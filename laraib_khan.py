# -*- coding: utf-8 -*-
"""Laraib_Khan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FDOYI4Ewj2FYXROf5z30zRC2jhWEeTCe
"""

import io, textwrap, pandas as pd, numpy as np, statsmodels.api as sm
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind, pearsonr
from statsmodels.tsa.stattools import adfuller

raw = textwrap.dedent("""
year\tpopulation\tgdp\tcement_co2\tco2\tco2_per_gdp\tco2_per_unit_energy\tcoal_co2\tconsumption_co2_per_gdp\tenergy_per_gdp\tgas_co2\toil_co2\tprimary_energy_consumption\ttrade_co2\ttrade_co2_share\texports\tCBAM Cost\tAHS Dutiable Imports
2003\t167110250\t5.4392E+11\t5.171\t117.462\t0.216\t0.21\t13.165\t0.221\t1.029\t54.912\t44.213\t559.927\t2.921\t2.487\t2651702.3\t97.75\t738854.47
2004\t171285998\t5.92026E+11\t5.604\t129.796\t0.219\t0.208\t17.078\t0.239\t1.052\t60.778\t46.335\t623.062\t11.816\t9.103\t2953805.74\t97.75\t1126036.24
2005\t175453212\t6.41139E+11\t6.639\t133.645\t0.208\t0.207\t15.147\t0.228\t1.008\t63.495\t48.363\t646.484\t12.385\t9.267\t3454139.9\t97.75\t1261434.7
2006\t179682692\t6.85966E+11\t7.887\t144.053\t0.21\t0.211\t18.191\t0.23\t0.993\t63.756\t54.219\t681.332\t13.433\t9.325\t3673078.5\t97.75\t1346384.3
2007\t184493230\t7.19254E+11\t9.465\t157.678\t0.219\t0.218\t25.384\t0.237\t1.007\t64.952\t57.877\t724.411\t12.587\t7.983\t4009161.37\t97.75\t1883176.54
2008\t189499114\t7.46913E+11\t10.33\t155.149\t0.208\t0.213\t20.137\t0.226\t0.975\t65.754\t58.928\t727.998\t13.84\t8.921\t4029014.92\t97.75\t1028100.38
2009\t194376535\t7.74881E+11\t11.611\t155.521\t0.201\t0.211\t18.613\t0.216\t0.952\t66.989\t58.309\t737.606\t11.715\t7.533\t3458428.34\t97.75\t1023926.32
2010\t199239047\t8.05492E+11\t11.214\t154.202\t0.191\t0.208\t17.719\t0.206\t0.92\t63.123\t62.145\t740.76\t11.926\t7.734\t4323540.07\t97.75\t1545196.01
2011\t203746069\t8.45097E+11\t11.022\t154.801\t0.183\t0.209\t17.316\t0.2\t0.876\t65.959\t60.504\t740.263\t14.073\t9.091\t5394504.59\t97.75\t1983389.35
2012\t207667129\t8.78485E+11\t11.391\t154.768\t0.176\t0.225\t14.579\t0.195\t0.783\t64.179\t63.618\t687.836\t16.41\t10.603\t4312356.31\t97.75\t1665095.18
2013\t211073979\t9.18329E+11\t11.962\t151.847\t0.165\t0.202\t13.985\t0.18\t0.819\t63.719\t61.041\t751.742\t13.66\t8.996\t4968625.73\t97.75\t1832924.02
2014\t214264649\t9.61517E+11\t12.198\t156.705\t0.163\t0.202\t19.02\t0.18\t0.807\t64.201\t60.423\t775.935\t16.484\t10.519\t5692289.69\t97.75\t2874351.85
2015\t217290885\t1010825297920.000\t12.704\t150.59\t0.149\t0.185\t18.206\t0.172\t0.807\t64.406\t54.377\t815.859\t22.855\t15.177\t5389913.18\t97.75\t2996569.32
2016\t220138862\t1061146787840.000\t14.128\t178.032\t0.168\t0.328\t24.233\t0.192\t0.511\t64.522\t74.334\t542.201\t25.396\t14.265\t5787474.66\t97.75\t3071879.26
2017\t223273975\t1117267165184.000\t14.752\t218.361\t0.195\t0.232\t41.29\t0.217\t0.842\t77.919\t83.719\t940.998\t24.154\t11.061\t6355863.4\t97.75\t0
2018\t226928893\t1165406502912.000\t15.581\t210.348\t0.18\t0.217\t47.12\t0.2\t0.833\t80.049\t67.023\t971.03\t22.229\t10.568\t6643941.99\t97.75\t3682060.01
2019\t230800909\t1172330119168.000\t15.437\t207.622\t0.177\t0.211\t50.529\t0.195\t0.838\t79.284\t61.91\t982.516\t20.741\t9.99\t6725186.15\t97.75\t2854883.51
2020\t235001743\t1202026446848.000\t17.645\t230.754\t0.192\t0.236\t61.814\t0.206\t0.813\t70.557\t80.284\t977.217\t17.257\t7.479\t6508751.49\t97.75\t1867385.74
2021\t239477805\t1278204968960.000\t18.621\t247.901\t0.194\t0.229\t63.31\t0.211\t0.848\t70.36\t95.169\t1083.791\t21.607\t8.716\t8070929.3\t97.75\t2792124.8
2022\t243700667\t1318834929664.000\t16.917\t223.834\t0.17\t0.224\t56.305\t0.188\t0.757\t59.875\t90.286\t998.705\t23.946\t10.698\t9115212.61\t97.75\t1867385.74
2023\t247504504\t\t15.971\t200.667\t\t0.214\t54.056\t\t\t59.314\t70.899\t936.701\t\t\t1695933.74\t\t\t
""")

# robust parse
import re
lines = [ln for ln in raw.strip().splitlines()][1:]
rows = []
for ln in lines:
    tokens = re.findall(r'[-+]?\d*\.?\d+E[-+]?\d+|[-+]?\d*\.\d+|[-+]?\d+', ln, flags=re.IGNORECASE)
    nums = [float(t) for t in tokens]
    # pad if needed (shouldn't be necessary here)
    if len(nums) < 18:
        nums += [np.nan]*(18-len(nums))
    rows.append(nums)

cols = ["year","population","gdp","cement_co2","co2","co2_per_gdp","co2_per_unit_energy","coal_co2",
        "consumption_co2_per_gdp","energy_per_gdp","gas_co2","oil_co2","primary_energy_consumption",
        "trade_co2","trade_co2_share","exports","CBAM_Cost_given","AHS_Dutiable_Imports"]

df = pd.DataFrame(rows, columns=cols)
for c in df.columns:
    df[c] = pd.to_numeric(df[c], errors='coerce')

df.head()

textile_share_of_trade_co2 = 0.54   # user-provided default; change if you have better estimate
CBAM_PRICE_USD_PER_T = 97.75        # user-provided constant

# trade_co2 assumed in million tonnes (MtCO2)
df["textile_trade_emissions_mton_B"] = df["trade_co2"] * textile_share_of_trade_co2
df["textile_trade_emissions_tonnes_B"] = df["textile_trade_emissions_mton_B"] * 1e6
df["CBAM_Cost_textile_B_USD"] = df["textile_trade_emissions_tonnes_B"] * CBAM_PRICE_USD_PER_T
df["CBAM_share_of_exports_B"] = df["CBAM_Cost_textile_B_USD"] / df["exports"]
df["emission_intensity_t_per_USD_tradeB"] = df["textile_trade_emissions_tonnes_B"] / df["exports"]

# log transforms
df["ln_exports"] = np.log(df["exports"].replace(0, np.nan))
df["ln_gdp"] = np.log(df["gdp"].replace(0, np.nan))

# EDA: summary and correlations
desc = df[["year","exports","trade_co2","textile_trade_emissions_mton_B","CBAM_Cost_textile_B_USD","CBAM_share_of_exports_B","emission_intensity_t_per_USD_tradeB","energy_per_gdp","gdp"]].describe()
corr_df = df[["exports","CBAM_Cost_textile_B_USD","CBAM_share_of_exports_B","emission_intensity_t_per_USD_tradeB","energy_per_gdp","gdp"]].corr()



import seaborn as sns
sns.heatmap(corr_df, annot = True)

# Plots saved
plt.figure(figsize=(10,5))
plt.plot(df["year"], df["exports"], label="Exports")
#plt.plot(df["year"], df["CBAM_Cost_textile_B_USD"], label="CBAM Cost (textile, method B)")
plt.xlabel("Year"); plt.title("Exports over Time"); plt.legend(); plt.grid(True); plt.tight_layout()
plt.savefig("/content/exports_vs_cbam_methodB.png")

# Plots saved
plt.figure(figsize=(10,5))
plt.plot(df["year"], df['ln_exports'], label="Exports")
#plt.plot(df["year"], df["CBAM_Cost_textile_B_USD"], label="CBAM Cost (textile, method B)")
plt.xlabel("Year"); plt.title("Exports over Time"); plt.legend(); plt.grid(True); plt.tight_layout()
plt.savefig("/content/exports_vs_cbam_methodB.png")

plt.figure(figsize=(10,5))
plt.plot(df["year"], df["CBAM_Cost_textile_B_USD"], label="CBAM Cost")
plt.xlabel("Year"); plt.title("CBAM Cost over Time"); plt.legend(); plt.grid(True); plt.tight_layout()

plt.figure(figsize=(10,5))
plt.plot(df["year"], df["emission_intensity_t_per_USD_tradeB"], label="Emission intensity (tCO2 per USD exported)")
plt.xlabel("Year"); plt.title("Emission intensity  over time"); plt.legend(); plt.grid(True); plt.tight_layout()

# Stationarity tests (ADF) on ln_exports and CBAM share
adf_ln_exports = adfuller(df["ln_exports"].dropna(), autolag='AIC')
adf_cbam_share = adfuller((df["CBAM_share_of_exports_B"]).dropna(), autolag='AIC')



adf_ln_exports

adf_cbam_share

# Regression: Levels (ln(exports)) and Growth (delta ln(exports))
reg_df = df.dropna(subset=["ln_exports","emission_intensity_t_per_USD_tradeB","energy_per_gdp","ln_gdp","CBAM_share_of_exports_B"])
X = reg_df[["emission_intensity_t_per_USD_tradeB","energy_per_gdp","ln_gdp","CBAM_share_of_exports_B"]]
X = sm.add_constant(X)
y = reg_df["ln_exports"]
model_levels = sm.OLS(y, X).fit(cov_type='HC1')



print(model_levels.summary())

import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller

# -------------------------------
# 1. Drop missing observations
# -------------------------------
reg_df = df.dropna(subset=[
    "ln_exports",
    "emission_intensity_t_per_USD_tradeB",
    "energy_per_gdp",
    "ln_gdp",
    "CBAM_share_of_exports_B"
]).copy()

# -------------------------------
# 2. ADF unit-root tests
# -------------------------------
def adf_test(series, name):
    res = adfuller(series, autolag="AIC")
    print(f"\nADF Test — {name}")
    print(f"Test statistic: {res[0]:.4f}")
    print(f"P-value       : {res[1]:.4f}")
    for k, v in res[4].items():
        print(f"  {k}: {v:.4f}")

# Levels
adf_test(reg_df["ln_exports"], "ln_exports (level)")
adf_test(reg_df["ln_gdp"], "ln_gdp (level)")

# First differences
adf_test(reg_df["ln_exports"].diff().dropna(), "Δ ln_exports")
adf_test(reg_df["ln_gdp"].diff().dropna(), "Δ ln_gdp")

# -------------------------------
# 3. First differences
# -------------------------------
df_diff = reg_df.diff()

# -------------------------------
# 4. Create lagged first differences
# -------------------------------
x_vars = [
    "emission_intensity_t_per_USD_tradeB",
    "energy_per_gdp",
    "ln_gdp",
    "CBAM_share_of_exports_B"
]

for var in x_vars:
    df_diff[f"{var}_lag1"] = df_diff[var].shift(1)

# -------------------------------
# 5. Regression dataset
# -------------------------------
df_model = df_diff.dropna().copy()

y = df_model["ln_exports"]

X = df_model[
    x_vars + [f"{var}_lag1" for var in x_vars]
]

X = sm.add_constant(X)

# -------------------------------
# 6. Estimate dynamic FD model
# Δy_t = βΔX_t + γΔX_{t−1} + u_t
# -------------------------------
model = sm.OLS(y, X).fit(
    cov_type="HAC",
    cov_kwds={"maxlags": 1}
)

print(model.summary())

# -------------------------------
# 7. Total short-run effects
# β + γ
# -------------------------------
print("\nTotal short-run effects (ΔX_t + ΔX_{t−1}):")
for var in x_vars:
    total_effect = model.params[var] + model.params[f"{var}_lag1"]
    print(f"{var}: {total_effect:.4f}")

# -------------------------------
# 8. Joint significance tests
# H0: β + γ = 0
# -------------------------------
for var in x_vars:
    print(f"\nJoint test for {var}:")
    print(model.t_test(f"{var} + {var}_lag1 = 0"))

import pandas as pd
import statsmodels.api as sm
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.stattools import adfuller

# --------------------------------------------------
# 1. Prepare data
# --------------------------------------------------
reg_df = df.dropna(subset=[
    "ln_exports",
    "emission_intensity_t_per_USD_tradeB",
    "energy_per_gdp",
    "ln_gdp",
    "CBAM_share_of_exports_B"
]).copy()

y = reg_df["ln_exports"]

X = reg_df[
    ["emission_intensity_t_per_USD_tradeB",
     "energy_per_gdp",
     "ln_gdp",
     "CBAM_share_of_exports_B"]
]

# --------------------------------------------------
# 2. Check stationarity of dependent variable
# --------------------------------------------------
adf = adfuller(y)
print("ADF p-value (ln_exports):", adf[1])

# If p-value > 0.05 → use d=1
d = 1 if adf[1] > 0.05 else 0

# --------------------------------------------------
# 3. Fit ARIMAX model
#    (p,d,q) can be adjusted or grid-searched
# --------------------------------------------------
model = SARIMAX(
    y,
    exog=X,
    order=(1, d, 1),          # ARIMA(p,d,q)
    enforce_stationarity=False,
    enforce_invertibility=False
)

results = model.fit(disp=False)

print(results.summary())

# --------------------------------------------------
# 4. Diagnostics
# --------------------------------------------------
results.plot_diagnostics(figsize=(12, 8))

# --------------------------------------------------
# 5. Short-run interpretation helper
# --------------------------------------------------
print("\nExogenous variable coefficients:")
for var in X.columns:
    print(f"{var}: {results.params[var]:.4f}")

# Growth form: Δln(exports) on Δvariables
df_sorted = df.sort_values("year").reset_index(drop=True)
df_sorted["dln_exports"] = df_sorted["ln_exports"].diff()
df_sorted["demission_intensity"] = df_sorted["emission_intensity_t_per_USD_tradeB"].diff()
df_sorted["denergy_per_gdp"] = df_sorted["energy_per_gdp"].diff()
df_sorted["dln_gdp"] = df_sorted["ln_gdp"].diff()
df_sorted["dCBAM_share"] = df_sorted["CBAM_share_of_exports_B"].diff()

reg_gdf = df_sorted.dropna(subset=["dln_exports","demission_intensity","denergy_per_gdp","dln_gdp","dCBAM_share"])
Xg = reg_gdf[["demission_intensity","denergy_per_gdp","dln_gdp","dCBAM_share"]]
Xg = sm.add_constant(Xg)
yg = reg_gdf["dln_exports"]
model_growth = sm.OLS(yg, Xg).fit(cov_type='HC1')

print(model_growth.summary())

from scipy.stats import ttest_ind

print("MEDIAN-SPLIT WELCH T-TEST ON EMISSION INTENSITY\n")

# Step 1: Median split
median_int = df["emission_intensity_t_per_USD_tradeB"].median()
print(f"Median emission intensity = {median_int:.4f}\n")

# Step 2: Create groups
high_group = df[df["emission_intensity_t_per_USD_tradeB"] >= median_int]["exports"].dropna()
low_group  = df[df["emission_intensity_t_per_USD_tradeB"] < median_int]["exports"].dropna()

print(f"High emission group: {len(high_group)} observations")
print(f"Low emission group : {len(low_group)} observations\n")

print(f"Mean exports (high emission): {high_group.mean():.4f}")
print(f"Mean exports (low emission) : {low_group.mean():.4f}\n")

# Step 3: Welch t-test
t_stat, p_val = ttest_ind(
    high_group,
    low_group,
    equal_var=False,
    nan_policy="omit"
)

print("Hypothesis being tested:")
print("H0: Mean exports are equal in high- and low-emission-intensity groups")
print("H1: Mean exports are different between the two groups\n")

print(f"T-statistic = {t_stat:.4f}")
print(f"P-value     = {p_val:.4f}\n")

# Step 4: Decision rule
alpha = 0.05
if p_val < alpha:
    print(f"Decision: Reject H0 at α = {alpha}")
    print("Interpretation: Exports differ significantly between high- and low-emission-intensity observations.")
else:
    print(f"Decision: Fail to reject H0 at α = {alpha}")
    print("Interpretation: No statistically significant difference in exports between the two groups.")

print("\nNotes:")
print("- This is Welch’s t-test (does not assume equal variances).")
print("- This is a descriptive comparison, not a causal analysis.")
print("- Time-series dependence is ignored.")



# Pearson correlation between CBAM share and exports
corr_cbam_exports, p_cbam_exports = pearsonr(df["CBAM_share_of_exports_B"].fillna(0), df["exports"].fillna(0))

# Monte-Carlo sensitivity: vary textile_share_of_trade_co2 (0.3-0.7) and CBAM price (50-150)
n_sims = 5000
np.random.seed(42)
share_samples = np.random.uniform(0.3,0.7,size=n_sims)
price_samples = np.random.uniform(50,150,size=n_sims)

# For each year, compute distribution of percent export revenue loss = CBAM_cost / exports
mc_results = {}
for idx, row in df.iterrows():
    exports = row["exports"]
    trade_co2 = row["trade_co2"]  # Mt
    if np.isnan(exports) or np.isnan(trade_co2):
        mc_results[row["year"]] = None
        continue
    # compute losses for all sims
    textile_emissions_t = (trade_co2 * share_samples) * 1e6  # tonnes
    cbam_costs = textile_emissions_t * price_samples  # USD
    loss_share = cbam_costs / exports  # fraction of export value
    mc_results[row["year"]] = {
        "median_loss_pct": np.median(loss_share)*100,
        "p5_loss_pct": np.percentile(loss_share,5)*100,
        "p95_loss_pct": np.percentile(loss_share,95)*100
    }

# Save processed CSV and plots
out_csv = "/content/pakistan_cbam_methodB_processed.csv"
df.to_csv(out_csv, index=False)

# Print concise results
print("=== Assumptions ===")
print(f"Method B: textile_share_of_trade_co2 = {textile_share_of_trade_co2} (default)")
print(f"CBAM price used (baseline): {CBAM_PRICE_USD_PER_T} USD/tCO2\n")

print("=== Descriptive summary (selected) ===")
print(desc.to_string())
print("\n=== Correlation matrix (selected vars) ===")
print(corr_df.to_string())

print("\n=== ADF Tests ===")
print(f"ADF ln(exports): test-stat = {adf_ln_exports[0]:.4f}, p-value = {adf_ln_exports[1]:.4f}")
print(f"ADF CBAM_share (method B): test-stat = {adf_cbam_share[0]:.4f}, p-value = {adf_cbam_share[1]:.4f}")

print("\n=== OLS Levels: ln(exports) ~ intensity + energy_per_gdp + ln(gdp) + CBAM_share_B ===")
print(model_levels.summary())

print("\n=== OLS Growth-form: Δln(exports) ~ Δintensity + Δenergy_per_gdp + Δln(gdp) + ΔCBAM_share ===")
print(model_growth.summary())

print("\n=== Median-split t-test on exports (high vs low emission intensity) ===")
print(f"median intensity (t per USD) = {median_int:.6e}")
print(f"t-statistic = {t_stat:.3f}, p-value = {p_val:.4f}")

print("\n=== Pearson correlation between CBAM share (B) and exports ===")
print(f"r = {corr_cbam_exports:.3f}, p = {p_cbam_exports:.4f}")

print("\n=== Monte-Carlo sensitivity (textile share 0.3-0.7, price 50-150 USD) ===")
for yr in sorted([y for y in mc_results.keys() if mc_results[y] is not None]):
    r = mc_results[yr]
    print(f"{yr}: median loss = {r['median_loss_pct']:.3f}%, 5th pct = {r['p5_loss_pct']:.3f}%, 95th pct = {r['p95_loss_pct']:.3f}%")

print(f"\nProcessed dataset saved to: {out_csv}")
#print("Plots saved to /mnt/data/exports_vs_cbam_methodB.png and /mnt/data/emission_intensity_methodB.png")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

n_sims = 5000
np.random.seed(42)

share_samples = np.random.uniform(0.3, 0.7, size=n_sims)
price_samples = np.random.uniform(50, 150, size=n_sims)

mc_full = []

for idx, row in df.iterrows():
    exports = row["exports"]
    trade_co2 = row["trade_co2"]
    year = row["year"]

    if np.isnan(exports) or np.isnan(trade_co2):
        continue

    textile_emissions_t = (trade_co2 * share_samples) * 1e6
    cbam_costs = textile_emissions_t * price_samples
    loss_share = cbam_costs / exports * 100  # percent

    temp = pd.DataFrame({
        "year": year,
        "textile_share": share_samples,
        "cbam_price": price_samples,
        "loss_pct": loss_share
    })

    mc_full.append(temp)

mc_df = pd.concat(mc_full, ignore_index=True)

plt.figure(figsize=(7,5))
plt.scatter(mc_df["textile_share"], mc_df["loss_pct"], alpha=0.05)
plt.xlabel("Textile share of trade CO₂")
plt.ylabel("Export revenue loss (%)")
plt.title("CBAM export loss vs textile emission share")
plt.grid(True)
plt.show()

plt.figure(figsize=(7,5))
plt.scatter(mc_df["cbam_price"], mc_df["loss_pct"], alpha=0.05)
plt.xlabel("CBAM carbon price (USD per tCO₂)")
plt.ylabel("Export revenue loss (%)")
plt.title("CBAM export loss vs CBAM carbon price")
plt.grid(True)
plt.show()

mc_df["price_bin"] = pd.cut(mc_df["cbam_price"], bins=[50,75,100,125,150])

price_summary = mc_df.groupby("price_bin")["loss_pct"].median()

price_summary.plot(kind="bar", figsize=(7,5))
plt.ylabel("Median export loss (%)")
plt.xlabel("CBAM price range (USD/tCO₂)")
plt.title("Median CBAM export loss by carbon price range")
plt.grid(axis="y")
plt.show()

share_bins = pd.cut(mc_df["textile_share"], bins=[0.3,0.4,0.5,0.6,0.7])
price_bins = pd.cut(mc_df["cbam_price"], bins=[50,75,100,125,150])

pivot = mc_df.pivot_table(
    values="loss_pct",
    index=share_bins,
    columns=price_bins,
    aggfunc="median"
)

plt.figure(figsize=(9,6))
plt.imshow(pivot, aspect="auto", origin="lower")
plt.colorbar(label="Median export loss (%)")
plt.xticks(range(len(pivot.columns)), pivot.columns.astype(str), rotation=45)
plt.yticks(range(len(pivot.index)), pivot.index.astype(str))
plt.xlabel("CBAM price range (USD/tCO₂)")
plt.ylabel("Textile share of trade CO₂")
plt.title("Median CBAM export loss (%) — joint sensitivity")
plt.show()

features = [
    "emission_intensity_t_per_USD_tradeB",
    "energy_per_gdp",
    "ln_gdp",
    "CBAM_share_of_exports_B"
]

df_ml = df.dropna(subset=["year", "ln_exports"] + features).copy()



results  = pd.DataFrame(mc_results)



results

# =====================================================
# FULL PIPELINE: ML forecasting + CI + CBAM scenarios
# =====================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.utils import resample

# -------------------------------
# 1. Setup
# -------------------------------
features = [
    "emission_intensity_t_per_USD_tradeB",
    "energy_per_gdp",
    "ln_gdp",
    "CBAM_share_of_exports_B"
]
target = "ln_exports"

np.random.seed(42)

# -------------------------------
# 2. Create 2-year lagged dataset
# -------------------------------
df_lag = df.sort_values("year").copy()

for var in features + [target]:
    df_lag[f"{var}_lag1"] = df_lag[var].shift(1)
    df_lag[f"{var}_lag2"] = df_lag[var].shift(2)

df_ml = df_lag.dropna().copy()

X_cols = []
for v in features:
    X_cols += [f"{v}_lag1", f"{v}_lag2"]

# -------------------------------
# 3. Train / test split
# -------------------------------
train_df = df_ml[df_ml["year"] < 2017]
test_df  = df_ml[df_ml["year"] >= 2017]

X_train, y_train = train_df[X_cols], train_df[target]
X_test,  y_test  = test_df[X_cols],  test_df[target]

# -------------------------------
# 4. Models (linear + non-linear)
# -------------------------------
models = {
    "OLS": LinearRegression(),
    "GradientBoosting": GradientBoostingRegressor(
        n_estimators=500,
        learning_rate=0.02,
        max_depth=3,
        random_state=42
    )
}

# -------------------------------
# 5. Train models
# -------------------------------
fitted_models = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    fitted_models[name] = model

# -------------------------------
# 6. Recursive future inputs to 2030
# -------------------------------
last_year = int(df_ml["year"].max())
future_years = list(range(last_year + 1, 2031))

future_data = df_lag[df_lag["year"] <= last_year].copy()

for yr in future_years:
    prev1, prev2 = future_data.iloc[-1], future_data.iloc[-2]

    new_row = {
        "year": yr,
        "ln_gdp": prev1["ln_gdp"] + 0.03,
        "energy_per_gdp": prev1["energy_per_gdp"] * 0.99,
        "emission_intensity_t_per_USD_tradeB": prev1["emission_intensity_t_per_USD_tradeB"] * 0.98,
        "CBAM_share_of_exports_B": prev1["CBAM_share_of_exports_B"]
    }

    for v in features:
        new_row[f"{v}_lag1"] = prev1[v]
        new_row[f"{v}_lag2"] = prev2[v]

    future_data = pd.concat([future_data, pd.DataFrame([new_row])], ignore_index=True)

future_df = future_data[future_data["year"].isin(future_years)]

# -------------------------------
# 7. CBAM scenarios (policy timing)
# -------------------------------
scenarios = {
    "No_CBAM": np.zeros(len(future_df)),
    "CBAM_from_2026": np.where(
        future_df["year"] >= 2026,
        future_df["CBAM_share_of_exports_B"],
        0.0
    ),
    "High_CBAM_from_2026": np.where(
        future_df["year"] >= 2026,
        1.5 * future_df["CBAM_share_of_exports_B"],
        0.0
    )
}

# -------------------------------
# 8. Bootstrap confidence intervals
# -------------------------------
def bootstrap_forecast(model, X_train, y_train, X_future, n_boot=300):
    preds = []
    for _ in range(n_boot):
        X_b, y_b = resample(X_train, y_train)
        model.fit(X_b, y_b)
        preds.append(model.predict(X_future))
    preds = np.array(preds)
    return (
        preds.mean(axis=0),
        np.percentile(preds, 5, axis=0),
        np.percentile(preds, 95, axis=0)
    )

# -------------------------------
# 9. Forecast + CI for each model/scenario
# -------------------------------
forecast_results = []

for model_name, model in fitted_models.items():
    for scen, cbam_path in scenarios.items():

        X_future = future_df[X_cols].copy()
        X_future["CBAM_share_of_exports_B_lag1"] = cbam_path
        X_future["CBAM_share_of_exports_B_lag2"] = pd.Series(cbam_path).shift(1).fillna(0).values

        mean_f, lo_f, hi_f = bootstrap_forecast(
            model, X_train, y_train, X_future
        )

        temp = pd.DataFrame({
            "year": future_df["year"],
            "Model": model_name,
            "Scenario": scen,
            "exports_mean": np.exp(mean_f),
            "exports_p5": np.exp(lo_f),
            "exports_p95": np.exp(hi_f)
        })

        forecast_results.append(temp)

forecast_df = pd.concat(forecast_results, ignore_index=True)

# -------------------------------
# 10. Plot export paths with CI
# -------------------------------
for model_name in forecast_df["Model"].unique():
    plt.figure(figsize=(9,6))

    for scen in scenarios.keys():
        d = forecast_df[
            (forecast_df["Model"] == model_name) &
            (forecast_df["Scenario"] == scen)
        ]

        plt.plot(d["year"], d["exports_mean"], label=scen)
        plt.fill_between(
            d["year"],
            d["exports_p5"],
            d["exports_p95"],
            alpha=0.2
        )

    plt.title(f"Export forecasts with CBAM scenarios ({model_name})")
    plt.xlabel("Year")
    plt.ylabel("Exports")
    plt.legend()
    plt.grid(True)
    plt.show()

# -------------------------------
# 11. Stability check: CBAM effect size
# -------------------------------
print("\n=== MODEL STABILITY CHECK (CBAM entry post-2026) ===")

for model_name in forecast_df["Model"].unique():
    base = forecast_df[
        (forecast_df["Model"] == model_name) &
        (forecast_df["Scenario"] == "No_CBAM")
    ]["exports_mean"].values

    cbam = forecast_df[
        (forecast_df["Model"] == model_name) &
        (forecast_df["Scenario"] == "CBAM_from_2026")
    ]["exports_mean"].values

    pct_change_2030 = (cbam[-1] - base[-1]) / base[-1] * 100

    print(f"{model_name}: Export change in 2030 due to CBAM = {pct_change_2030:.2f}%")

# =====================================================
# FULL PIPELINE: ML Evaluation + CBAM Forecasts to 2030
# =====================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import Lasso
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor

from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.utils import resample

np.random.seed(42)

# =====================================================
# 1. VARIABLES
# =====================================================

features = [
    "emission_intensity_t_per_USD_tradeB",
    "energy_per_gdp",
    "ln_gdp",
    "CBAM_share_of_exports_B"
]

target = "ln_exports"

# =====================================================
# 2. CREATE 2-YEAR LAGS
# =====================================================

df_lag = df.sort_values("year").copy()

for v in features + [target]:
    df_lag[f"{v}_lag1"] = df_lag[v].shift(1)
    df_lag[f"{v}_lag2"] = df_lag[v].shift(2)

df_ml = df_lag.dropna().copy()

X_cols = []
for v in features:
    X_cols += [f"{v}_lag1", f"{v}_lag2"]

# =====================================================
# 3. TRAIN / TEST SPLIT (MODEL EVALUATION ONLY)
# =====================================================

train_df = df_ml[df_ml["year"] < 2017]
test_df  = df_ml[df_ml["year"] >= 2017]

X_train, y_train = train_df[X_cols], train_df[target]
X_test,  y_test  = test_df[X_cols],  test_df[target]

# =====================================================
# 4. MODELS
# =====================================================

models = {
    "LASSO": Lasso(alpha=0.001, max_iter=5000),
    "KNN": KNeighborsRegressor(n_neighbors=5, weights="distance"),
    "SVM": SVR(kernel="rbf", C=10, epsilon=0.05),
    "RandomForest": RandomForestRegressor(
        n_estimators=500,
        max_depth=6,
        random_state=42
    )
}

# =====================================================
# 5. MODEL EVALUATION (RMSE, MSE, MAE)
# =====================================================

metrics = []
pred_store = {}

for name, model in models.items():
    model.fit(X_train, y_train)

    y_pred_test = model.predict(X_test)

    mse  = mean_squared_error(y_test, y_pred_test)
    rmse = np.sqrt(mse)
    mae  = mean_absolute_error(y_test, y_pred_test)

    metrics.append({
        "Model": name,
        "RMSE": rmse,
        "MSE": mse,
        "MAE": mae
    })

    pred_store[name] = {
        "year": test_df["year"].values,
        "actual": np.exp(y_test.values),
        "predicted": np.exp(y_pred_test)
    }

metrics_df = pd.DataFrame(metrics).sort_values("RMSE")
print("\n=== MODEL PERFORMANCE (TEST SET) ===")
print(metrics_df)

# =====================================================
# 6. PLOTS: ACTUAL vs PREDICTED
# =====================================================

for name, d in pred_store.items():
    plt.figure(figsize=(8,5))
    plt.plot(d["year"], d["actual"], label="Actual", linewidth=2)
    plt.plot(d["year"], d["predicted"], label="Predicted", linestyle="--")
    plt.title(f"Actual vs Predicted Exports ({name})")
    plt.xlabel("Year")
    plt.ylabel("Exports")
    plt.legend()
    plt.grid(True)
    plt.show()

# =====================================================
# 7. FUTURE FEATURES TO 2030 (RECURSIVE)
# =====================================================

last_year = int(df_ml["year"].max())
future_years = list(range(last_year + 1, 2031))

future_data = df_lag[df_lag["year"] <= last_year].copy()

for yr in future_years:
    prev1, prev2 = future_data.iloc[-1], future_data.iloc[-2]

    new_row = {
        "year": yr,
        "ln_gdp": prev1["ln_gdp"] + 0.03,
        "energy_per_gdp": prev1["energy_per_gdp"] * 0.99,
        "emission_intensity_t_per_USD_tradeB": prev1["emission_intensity_t_per_USD_tradeB"] * 0.98,
        "CBAM_share_of_exports_B": prev1["CBAM_share_of_exports_B"]
    }

    for v in features:
        new_row[f"{v}_lag1"] = prev1[v]
        new_row[f"{v}_lag2"] = prev2[v]

    future_data = pd.concat(
        [future_data, pd.DataFrame([new_row])],
        ignore_index=True
    )

future_df = future_data[future_data["year"].isin(future_years)]

# =====================================================
# 8. CBAM COST SCENARIOS
# =====================================================

cbam_multipliers = {
    "No_CBAM": 0.0,
    "Low_CBAM": 0.5,
    "Baseline_CBAM": 1.0,
    "High_CBAM": 1.5,
    "Extreme_CBAM": 2.0
}

# =====================================================
# 9. BOOTSTRAP CI FOR FUTURE FORECASTS (NO TEST DATA)
# =====================================================

def bootstrap_future_ci(model_class, model_params, X_train, y_train, X_future, n_boot=300):
    preds = []

    for _ in range(n_boot):
        X_b, y_b = resample(X_train, y_train)
        model = model_class(**model_params)
        model.fit(X_b, y_b)
        preds.append(model.predict(X_future))

    preds = np.array(preds)

    return (
        preds.mean(axis=0),
        np.percentile(preds, 5, axis=0),
        np.percentile(preds, 95, axis=0)
    )

# =====================================================
# 10. FORECASTS TO 2030 + CI
# =====================================================

forecast_results = []

for name, model in models.items():
    model_class = model.__class__
    model_params = model.get_params()

    for scen, mult in cbam_multipliers.items():
        X_future = future_df[X_cols].copy()

        cbam_path = mult * future_df["CBAM_share_of_exports_B"].values
        X_future["CBAM_share_of_exports_B_lag1"] = cbam_path
        X_future["CBAM_share_of_exports_B_lag2"] = (
            pd.Series(cbam_path).shift(1).fillna(0).values
        )

        mean_f, lo_f, hi_f = bootstrap_future_ci(
            model_class,
            model_params,
            X_train,
            y_train,
            X_future
        )

        forecast_results.append(pd.DataFrame({
            "year": future_df["year"],
            "Model": name,
            "Scenario": scen,
            "exports_mean": np.exp(mean_f),
            "exports_p5": np.exp(lo_f),
            "exports_p95": np.exp(hi_f)
        }))

forecast_df = pd.concat(forecast_results, ignore_index=True)

# =====================================================
# 11. PLOTS: FORECASTS WITH CI TO 2030
# =====================================================

for name in forecast_df["Model"].unique():
    plt.figure(figsize=(9,6))

    for scen in cbam_multipliers.keys():
        d = forecast_df[
            (forecast_df["Model"] == name) &
            (forecast_df["Scenario"] == scen)
        ]

        plt.plot(d["year"], d["exports_mean"], label=scen)
        plt.fill_between(
            d["year"],
            d["exports_p5"],
            d["exports_p95"],
            alpha=0.15
        )

    plt.title(f"Export Forecasts to 2030 with CBAM Costs ({name})")
    plt.xlabel("Year")
    plt.ylabel("Exports")
    plt.legend()
    plt.grid(True)
    plt.show()

# =====================================================
# 12. CBAM COST SENSITIVITY (2030)
# =====================================================

print("\n=== CBAM COST IMPACT ON EXPORTS IN 2030 ===")

for name in forecast_df["Model"].unique():
    base = forecast_df[
        (forecast_df["Model"] == name) &
        (forecast_df["Scenario"] == "No_CBAM")
    ]["exports_mean"].values[-1]

    for scen in ["Low_CBAM", "Baseline_CBAM", "High_CBAM", "Extreme_CBAM"]:
        val = forecast_df[
            (forecast_df["Model"] == name) &
            (forecast_df["Scenario"] == scen)
        ]["exports_mean"].values[-1]

        pct = (val - base) / base * 100
        print(f"{name} | {scen}: {pct:.2f}%")

# =====================================================
# 13. TEST METRICS (NO REFITTING – REUSE FITTED MODELS)
# =====================================================

print("\n=== TEST SET PERFORMANCE (NO REFITTING) ===")

rows = []

for name, model in models.items():
    # reuse already-fitted model
    y_pred_test = model.predict(X_test)

    mse  = mean_squared_error(y_test, y_pred_test)
    rmse = np.sqrt(mse)
    mae  = mean_absolute_error(y_test, y_pred_test)

    rows.append({
        "Model": name,
        "MSE": mse,
        "RMSE": rmse,
        "MAE": mae
    })

metrics_no_refit_df = pd.DataFrame(rows).sort_values("RMSE")
print(metrics_no_refit_df)

